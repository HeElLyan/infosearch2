# -*- coding: utf-8 -*-
"""2задание.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GHtRiMfWhp6U1AvPHWyetrWBS3sxTFx7
"""

from google.colab import drive
drive.mount('gdrive')

input_path = 'gdrive/My Drive/4курс/Инфопоиск/result/'
output_tokens_path = 'gdrive/My Drive/4курс/Инфопоиск/result/tokens.txt'
output_lemmas_path = 'gdrive/My Drive/4курс/Инфопоиск/result/lemmas.txt'

from enum import unique
from itertools import chain
import re
import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')
sw_nltk = stopwords.words('english')

def get_words_from_file(input):

    # with open(input) as source, open(output, 'w') as destination:
    with open(input) as source:
        #get the text from file
        text = source.read()
        #lower the text
        text = text.lower()
        #split words in text
        words = text.split()
        #get words without number
        words = [re.sub(r'\d', '', word) for word in words]
        words = [re.sub(r'amott/', '', word) for word in words]
        words = [re.sub(r'\b\w{1,2}\b', '', word) for word in words]
        words = [word.replace("'", '') for word in words]
        #reduce stopwords .,!;()[]-:"/\|$@^''&*%?
        words = [word.strip('.,!;()[]-:"/\|$@^''&*%?') for word in words]
        #reduce stopwords
        words = [word for word in words if word not in sw_nltk]
    return words


def get_tokens_from_multiple_files(input):

    all_words = []

    #get all words from 142 files
    for i in range(142):
      
        words = get_words_from_file(input_path + 'выкачка' + str(i + 1) + '.txt')
        all_words.append(words)

    #2d list to 1d list
    all_words = sum(all_words, [])

    #finding unique
    unique_words = []
    for word in all_words:
        if word not in unique_words:
            unique_words.append(word)

    #sort unique words
    unique_words.sort()
    unique_words.pop(0)

    # print(unique_words)
    return unique_words


def write_tokens_to_file(output, unique_words):
    with open(output, 'w') as destination:
        for unique_word in unique_words:
            destination.write(unique_word + '\n')


from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
nltk.download('wordnet')
 
lemmatizer = WordNetLemmatizer()

def get_lemmas(tokens):
    words_lemmas = {}

    for token in tokens:
        key = lemmatizer.lemmatize(token, 'v')
        # words_lemmas[key] = token
        words_lemmas.setdefault(key, [])
        words_lemmas[key].append(token)
        # words_lemmas.append(lemmatizer.lemmatize(token, 'v'))

    return words_lemmas


def write_lemmas_to_file(output, lemmas):
    with open(output, 'w') as destination:
        for i in range(len(lemmas)):
            pair = list(lemmas.items())[i]
          
            res = ''
            for i in range(len(pair[1])):
                res += str(pair[1][i])
                res += ' '

            destination.write(str(pair[0]) + ': ' + res + '\n')

tokens = get_tokens_from_multiple_files(input_path)
lemmas = get_lemmas(tokens)
print(lemmas)
write_tokens_to_file(output_tokens_path, tokens)
write_lemmas_to_file(output_lemmas_path, lemmas)